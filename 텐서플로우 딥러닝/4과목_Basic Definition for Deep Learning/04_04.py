##### 선형 회귀분석 (머신러닝에서의 선형회귀 모델 1)
# 
#  라. 머신러닝에서의 선형회귀 모델
#    - 머신러닝에서는 training data를 제일 잘 설명하는 직선을 찾는 것이 목적
#    - 머신러닝에서 가장 좋은 모형을 선택하기 위해서는 
#       + 적절한 모형을 선택해야 한다. (예측: 선형, 함수 등, 분류: 이진, 다진 등)
#    - 통계분석과 머신러닝의 가장 큰 차이 = 데이터의 수
#
#              통계분석           |                 머신러닝          
#   --------------------------------------------------------------------------
#     데이터를 나누지 않는다.          데이터를 training, test set으로 구분 
#     (전체를 가지고 모형 개발)        (training set으로 학습 후 test set으로 검정)
#     가설이 필요함                   데이터가 상태적으로 많아 가설 불필요
#
#    - X(feature)가 하나뿐이기 때문에 Hypothesis는 2차원 좌표평면 상의 일차 함수로 H(x) = Wx + b가 됨
#       + H(X): 가설(Hypothesis), W:가중치(Weight), B: 편의(Bias)
#     
#      1) Cost(=Loss) function
#          . Cost = Hypothesis에서 실제 데인터와의 차이(통계의 잔차)         
#          . How fit the line to our (training) data  
#                     
#              H(x) - y
#          단순 오차를 평균 내면 양수와 음수가 같이 나온다
#          원하는 것은 실제 값과 예측 값 사이의 거리이므로, 양수와 음수가 함께 존재해서는 안된다.
#
#              (H(x)-y)^2
#
#          제곱을 하게 되면 차이의 +/- 상관없이 차이가 클 때 Penalty(벌점)을 더 주고, 
#          차이가 작을 때 Penalty를 상대적으로 덜 주기 때문에 비용을 계산하기에 더 효과적이다.
#          
#          . 비용이 가장 적은 직선, 최고의 Hypotheisis를 찾을 때 사용하는 함수 
#
#          우리가 세운 가설과 실제 데이터가 얼마나 맞는지 확인하는 것
#          H(x) -y: 찬차 하지만 +,-일 수도 있기 때문에 (H(x)-y)^2를 해서 양수로 표현하여 확인다. 
#          차이가 클 때 더 큰 패널티를 주기도 한다.
#        
#          . W와 b의 값에 따라 cost가 달라짐, 즉 Cost Function은 W와 b에 대한 식
#       2) Cost 최소화하기
#          가) Hypothesis and cost
#              H(x) = Wx + b 
#              cost(W,b) = (1/n) Σ(H(x)-y)^2
#          나) Simplified hypotheiss
#              H(x) = Wx 
#              cost(W) = (1/n) Σ(H(x)-y)^2
#          다) What cost(W) looks like? 
#                    
#              X   |   Y 
#            ------------
#              1     1         --> W = 0, cost(W) = 4.67
#              2     2         --> W = 1, cost(W) = 0           ****   
#              3     3         --> W = 2, cost(W) = 4.67  
#

